---
title: "Intro Extras"
output: html_document
date: "2024-02-22"
---


Scientific progress requires building upon past results, and therefore requires confidence in past results. 
Issues arise when subsequent research is based upon weak foundations --i.e., studies with a limited capacity to be replicated because of questionable practices or over-generalisation.
Early significant results can dictate the direction of research and grow resistant to later contradictory results [@barto_dissemination_2012]; therefore, early diagnoses of overly confident results or previously unknown sources of variation becomes a priority.

In medical fields, a lack of replicability comes with direct monetary and well-being costs [@freedman_economics_2015].
Like the medical field, ecological studies can come with well-being costs to the study subjects [e.g., direct surgery/marking of the animal @Reinert1982; @Winne2006], as well as impacts on stakeholders stemming from management decisions.
<!-- There are fears that the lack of replicability will feed distrust of science more generally [@anvari_replicability_2018]. -->
Therefore, maximising replicability in ecology is key to minimising research waste [@grainger_evidence_2019] and the negative impacts on study subjects and systems.

It would be advantageous to understand which choices have a significant impact and whether we can account for differences in choice during comparisons.
An understanding of choice could better guide decisions during a study and potentially be used to gauge the robustness of a given dataset in answering a given question.

The variation in final results can be considered originating from six sources of uncertainty/variation [Table \@ref(tab:sourcesVariation), @hoffmann_multiplicity_2021].

```{r sourcesVariationPrep, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(dplyr)

variationTable <- tribble(
  ~Origin, ~Details, ~Example,
  "Measurement", "Randomness from the act of measuring", "Measurement error stemming from imperfect precision or accuracy",
  "Data preprocessing", "Decisions on data inclusion/exclusion and transforming", "Differing thresholds for what counts as outlying",
  "Parameter", "Decisions on which parameters used as covariates/predictors", "Differing thresholds on acceptable covariance of predictors",
  "Model", "Decisions on model structure and specification", "Model formulation and specification of random effects",
  "Method", "Decisions on method choice and parameterisation", "Choosing an overarching GLMM over a series of T-tests",
  "Sampling", "Randomness as a result of sampling a wider population",  "Sampling from a wider population results in different samples"
)
```

```{r sourcesVariation, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(variationTable,
             format = "latex",
             align = "lcc",
             vline = "",
             toprule = "",
             # midrule = "",
             linesep = "",
             bottomrule = "",
             caption = "The potential sources leading to variation in results (Hoffmann et al., 2021).") %>%
  kableExtra::kable_styling(font_size = 8, latex_options = "scale_down")

```

Several sources of variation (data preprocessing, parameter, model, and method) are likely to be particularly key to defining researcher degrees of freedom post data collection.
In some cases, the cause behind the variability in results is hard to diagnose [@breznau_observing_2021], or will be less likely to be questioned because of the agreement with existing theory [@gelman_garden_2013].


If the data entropy [the process in which as data ages the chances of irreversible loss increase, @vines_availability_2014] and resistance to data sharing [@tedersoo_data_2021; @miyakawa_no_2020] can be overcome, we can retroactively explore the impact of researcher degrees of freedom on ecological studies [@rijnhart_assessing_2021].
Such retroactive assessment is an attractive option when other methods to explore false positive rates [@hoffmann_multiplicity_2021], such as preregistration and registered reports [@kaplan_likelihood_2015; @scheel_excess_2021], will require more time to yield results.
Ideally we can use multiverse analysis with preregistrations to boost transparency surrounding the inclusion of decisions and the rationale behind others exclusions [@simonsohn_specification_2020; @dragicevic_increasing_2019].
Given the success of meta-analyses to overcome short-comings in the publication record [e.g., p-hacking; @head_extent_2015], multiverse analysis may aid the direction of future research efforts by providing a means of meeting calls to replicate results before collecting more [@nuijten_verify_2018].



The data analysed (i.e., derived data, such as step length, speed, and turn angle derived from timestamped coordinate data) require multiple stages of preprocessing.
Therefore, multiverse analysis is an avenue to explore causes of variation between studies without the additional costs of practical studies, while also being capable of exploring data processing decisions that may not have immediately apparent impacts on final results.


They chiefly come in three forms: HARKing, cherry-picking, and p-hacking.
Hypothesising After Results Known (HARKing) is where the research can present the results as a confirmatory result, despite originally there being no or contrary hypothesis.
HARKing can sometimes be further enabled and rationalised by hindsight bias, where unexpected results are perceived as more likely once they have been observed [@forstmeier_detecting_2017; @gelman_garden_2013].
Cherry-picking is the removal or non-reporting of data points or (co-)variables, that did not yield significant results.
P-hacking is the repeated use of statistical tests, with different settings, to achieve statistical significance.
Arguably, the existence of p-hacking is enabled by an over-reliance on p-value thresholds, rather than flexible p-value thresholds that are predefined based on effect size of interest, sample size, and desired accuracy of estimation [@lakens_justify_2018].
